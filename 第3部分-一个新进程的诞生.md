# 第3部分-一个新进程的诞生

## 21 新进程诞生全局概述

### 21.1 整体代码

你看，第一部分和第二部分，为我们这个第三部分做了充足的铺垫工作。

到了第三部分，简单说就是**从内核态切换到用户态**，然后通过 **fork 创建出一个新的进程**，再之后**老进程进入死循环**。

```c
void main(void) {
    // 第二部分的内容，各种初始化工作
    ...
    // 第三部分的内容，一个新进程的诞生
    move_to_user_mode();
    if (!fork()) {
        // 新进程里干了啥，是第四部分的内容
        init();
    }
    // 死循环，操作系统怠速状态
    for(;;) pause();
}
```

至于 fork 出来的新进程做了什么事，就是 init 函数里的故事里，这个不在第三部分的讨论范畴。 



所以你看，一共就两行代码，顶多再算上最后一行的死循环，三行，就把创建新进程这个事搞定了。再加上新进程里要做的 init 函数，一共四行代码，就走到了 main 函数的结尾，也就标志着操作系统启动完毕！



但就是这没有多少个字母的四行代码，是整个操作系统的精髓所在，也是最难的四行代码。理解了它们，你就会有原来操作系统就是这破玩意的感叹了～







### 21.2 move_to_user_mode

直译过来即可，就是转变为用户态模式。因为 Linux 将**操作系统特权级**分为**用户态**与**内核态**两种，之前都处于内核态，现在要先转变为用户态，仅此而已。 



一旦转变为了用户态，那么之后的代码将一直处于用户态的模式，除非发生了中断，比如用户发出了系统调用的中断指令，那么此时将会从用户态陷入内核态，不过当中断处理程序执行完之后，又会通过中断返回指令从内核态回到用户态。

![图片](assets/640-1705897095853.png)

整个过程被操作系统的机制拿捏的死死的，**始终让用户进程处于用户态运行，必要的时候陷入一下内核态，但很快就会被返回而再次回到用户态**，是不是非常无奈？







### 21.3 fork

这是**创建一个新进程**的意思，而且所有用户进程想要创建新的进程，都需要调用这个函数。



原来操作系统只有一个**执行流**，就是我们一直看过来的所有代码，就是**进程 0**，只不过我们并没有意识到它也是一个进程。**调用完 fork** 之后，现在又多了一个进程，叫做**进程 1**。



当然，更准确的说法是，我们一路看过来的代码能够被我们自信地称作进程 0 的确切时刻，是我们在 [第18回 | 进程调度初始化 sched_init]() 里为**当前执行流**添加了**一个进程管理结构**到 **task 数组**里，同时开启了**定时器**以及**时钟中断**的那一个时刻。

![图片](assets/640-1705903866283-rs.png)

因为此时，**时钟中断**到来之后，就可以执行到我们的**进程调度程序**，进程调度程序才会去这个 task 数组里挑选合适的进程进行切换。

所以此时，我们当前执行的代码，才真正有了一个进程的身份，才勉强得到了一个可以被称为进程 0 的资格，毕竟还没有其他进程参与竞争。







### 21.4 init

只有**进程 1** 会走到这个分支来执行。这里的代码可太多了

- 它**本身需要完成如加载根文件系统的任务**
- 同时这个方法将**又会创建出一个新的进程 2**
- 在**进程 2** 里又会**加载与用户交互的 shell 程序**

此时操作系统就正式成为了用户可用的一个状态了。



当然，当你知道了新进程诞生的过程之后，进程 2 的创建，就和进程 1 的创建一样了，在后面的章节中你将不会再困惑创建新进程的过程，减轻了学习负担。所以这一部分，又是作为下一部分的重要基础，环环相扣。







### 21.5 pause

当没有任何可运行的进程时，操作系统会悬停在这里，达到怠速状态。没啥好说的，我一直强调，操作系统就是由中断驱动的一个死循环。



一共四句话，切换到用户态，创建新进程，初始化，然后悬停怠速。











## 22 从内核态切换到用户态

### 22.1 move_to_user_mode

```c
void main(void) {
    ...    
    move_to_user_mode();
    ...
}
```

这行代码的意思直接说非常简单，就是**从内核态转变为了用户态**，**用户进程**都在**用户态**这个**特权级**下运行，而有时程序想要做一些内核态才允许做的事情，比如读取硬盘的数据，就需要通过系统调用，来请求操作系统在内核态特权级下执行一些指令。 







### 22.2 让进程无法逃出用户态

我们现在的代码，还是在内核态下运行，之后操作系统达到怠速状态时，是以用户态的 shell 进程运行，随时等待着来自用户输入的命令。

所以，就在这一步，也就是 move_to_user_mode 这行代码，作用就是将当前代码的特权级，从内核态变为用户态。

一旦转变为了用户态，那么之后的代码将一直处于用户态的模式，除非发生了中断，比如用户发出了系统调用的中断指令，那么此时将会从用户态陷入内核态，不过当中断处理程序执行完之后，又会通过中断返回指令从内核态回到用户态。

![图片](assets/640-1705904821812.png)

整个过程被操作系统的机制拿捏的死死的，始终让用户进程处于用户态运行，必要的时候陷入一下内核态，但很快就会被返回而再次回到用户态，是不是非常无奈？这样操作系统就掌控了控制权，而用户进程再怎么折腾也无法逃出这个模式。









### 22.3 内核态与用户态的本质-特权级

首先从一个最大的视角来看，这一切都源于 CPU 的保护机制。CPU 为了配合操作系统完成保护机制这一特性，分别设计了**分段保护机制**与**分页保护机制**。

当我们在 [第七回 | 六行代码就进入了保护模式]() 将 cr0 寄存器的 PE 位开启时，就开启了保护模式，也即开启了**分段保护机制**。

![图片](assets/640-1705905239511-rs.png)

当我们在 [第九回 | Intel 内存管理两板斧：分段与分页]() 将 cr0 寄存器的 PG 位开启时，就开启了分页模式，也即开启了**分页保护机制**。

![图片](assets/640-1705905268279-rs.png)

有**关特权级的保护**，实际上**属于分段保护机制的一种**。具体怎么保护的呢？由于这里的细节比较繁琐，所以我举个例子简单理解下即可，实际上的特权级检查规则要比我说的多好多内容。



我们目前**正在执行的代码地址**，是通过 CPU 中的两个寄存器 `cs : eip` 指向的对吧？**cs 寄存器**是代码段寄存器，里面存着的是**段选择子**，还记得它的结构么？

![图片](assets/640-1705905398558-rs.png)

这里面的低端两位，此时表示 **CPL**，也就是**当前所处的特权级**，假如我们现在这个时刻，CS 寄存器的后两位为 3，二进制就是 11，就表示是当前**处理器处于用户态这个特权级**。

假如我们此时要跳转到另一处内存地址执行，在最终的汇编指令层面无非就是 jmp、call 和中断。我们拿 jmp 跳转来举例。

- 如果是短跳转，也就是直接 `jmp xxx`，那不涉及到段的变换，也就没有特权级检查这回事。

- 如果是长跳转，也就是 `jmp yyy : xxx`，这里的 yyy 就是另一个要跳转到的段的段选择子结构。

  ![图片](assets/640-1705905599235-rs.png)

  这个结构仍然是一样的段选择子结构，只不过这里的低端两位，表示 **RPL**，也就是**请求特权级，表示我想请求的特权级是什么**。同时，CPU 会拿这个段选择子去全局描述符表中寻找段描述符，从中找到段基址。

  ![图片](assets/640-1705905699904-rs.png)

  那还记得段描述符的样子么？

  ![图片](assets/640-1705905735614-rs.png)

  你看，这里面又有个 **DPL**，这表示**目标代码段特权级**，也就是即将要跳转过去的那个段的特权级。

  好了，我们总结一下简图，就是这三个玩意的比较。

  ![图片](assets/640-1705905821740-rs.png)

  这里的检查规则比较多，简单说，绝大多数情况下，**要求 CPL 必须等于 DPL**，才会跳转成功，否则就会报错。

  也就是说，当前代码所处段的特权级，必须要等于要跳转过去的代码所处的段的特权级，那就只能**用户态往用户态跳**，**内核态往内核态跳**，这样就防止了处于用户态的程序，跳转到内核态的代码段中做坏事。



这只是代码段跳转时所做的**特权级检查**，还有访问内存数据时也会有数据段的**特权级检查**，这里就不展开了。最终的效果是，**处于内核态的代码可以访问任何特权级的数据段，处于用户态的代码则只可以访问用户态的数据段**，这也就实现了内存数据读写的保护。



说了这么多，其实就是，**代码跳转只能同特权级，数据访问只能高特权级访问低特权级**。







### 22.4 特权级转换方式

诶不对呀，那我们今天要讲的是，从内核态转变为用户态，那如果代码跳转只能同特权级跳，我们现在处于内核态，要怎么样才能跳转到用户态呢？

Intel 设计了好多种**特权级转换的方式**，**中断**和**中断返回**就是其中的一种。

就是刚刚的图所表示的。

![图片](assets/640-1705906131115.png)

而**系统调用**就是这么玩的，用户通过 **int 0x80** 中断指令触发了中断，CPU 切换至内核态，执行中断处理程序，之后中断程序返回，又从内核态切换回用户态。



但有个问题是，我们当前的代码，此时就是处于内核态，并不是由一个用户态程序通过中断而切换到的内核态，那怎么回到原来的用户态呢？答案还是，通过**中断返回**。







### 22.5 中断返回--改变特权级

没有中断也能**中断返回**？可以的，Intel 设计的 CPU 就是这样不符合人们的直觉，中断和中断返回的确是应该配套使用的，但也可以单独使用，我们看代码。

`include\asm\system.h`

```c
#define move_to_user_mode() \
_asm { \
    _asm mov eax,esp \
    _asm push 00000017h \
    _asm push eax \
    _asm pushfd \
    _asm push 0000000fh \
    _asm push offset l1 \
    _asm iretd /* 执行中断返回指令*/ \
_asm l1: mov eax,17h \
    _asm mov ds,ax \
    _asm mov es,ax \
    _asm mov fs,ax \
    _asm mov gs,ax \
}

```

你看，这个方法里直接就执行了**中断返回指令 iretd**。



那么为什么之前进行了一共**五次的压栈操作呢**？因为中断返回理论上就是应该和中断配合使用的，而此时并不是真的发生了中断到这里，所以我们得**假装发生了中断**才行。

怎么假装呢？其实就把**栈**做做工作就好了，**中断发生时，CPU 会自动帮我们做如下的压栈操作。而中断返回时，CPU 又会帮我们把压栈的这些值返回赋值给响应的寄存器。**

![图片](assets/640-1705907946423.png)

去掉错误码，刚好是五个参数，所以我们在代码中模仿 CPU 进行了五次压栈操作，这样在执行 iretd 指令时，硬件会按顺序将刚刚压入栈中的数据，分别赋值给 `SS`、`ESP`、`EFLAGS`、`CS`、`EIP` 这几个寄存器，这就感觉像是正确返回了一样，让其**误以为这是通过中断进来的**。

- 压入栈的 `CS` 和 `EIP` 就表示中断发生前代码所处的位置，这样中断返回后好继续去那里执行。
- 压入栈的 `SS` 和 `ESP` 表示中断发生前的栈的位置，这样中断返回后才好恢复原来的栈。

其中，**特权级的转换**，就体现在 `CS 和 SS 寄存器`的值里，都是细节！



CS 和 SS 寄存器是段寄存器的一种，段寄存器里的值是段选择子

![图片](assets/640-1705908276336-rs.png)

对着这个结构，我们看代码。

`include\asm\system.h`

```c
#define move_to_user_mode() \
_asm { \
    _asm mov eax,esp \
    _asm push 00000017h \ ; 给 SS 赋值
    _asm push eax \
    _asm pushfd \
    _asm push 0000000fh \ ; 给 CS 赋值
    _asm push offset l1 \
    _asm iretd /* 执行中断返回指令*/ \
_asm l1: mov eax,17h \
    _asm mov ds,ax \
    _asm mov es,ax \
    _asm mov fs,ax \
    _asm mov gs,ax \
}
```

拿 CS 举例，给它赋的值是，`0000000fh`，用二进制表示为：`0000000000001111`

最后两位 11 表示特权级为 3，即用户态。而我们刚刚说了，CS 寄存器里的特权级，表示 CPL，即当前处理器特权级。

**所以经过 iretd 返回之后，CS 的值就变成了它，而当前处理器特权级，也就变成了用户态特权级。**







### 22.6 iretd&下一步

**除了改变特权级之外**

除了改变了特权级之外，还做了什么事情呢？

刚刚我们关注段寄存器，只关注了特权级的部分，我们再详细看看。

刚刚说了 CS 寄存器为 **0000000000001111**，最后两位表示用户态的含义。

![图片](assets/640-1705909113482-rs.png)

那继续解读，倒数第三位 **TI 表示**，前面的描述符索引，是从 GDT 还是 LDT 中取，**1 表示 LDT，也就是从局部描述符表中取。**

前面的描述符索引为 1，表示从局部描述符表中取到代码段描述符，如果你熟悉前面我讲过的内容，你将会直接得出上述结论。不过我还是帮你回忆一下。

在 [第18回 | 大名鼎鼎的进程调度就是从这里开始的]() 中，将 0 号 LDT 作为当前的 LDT 索引，记录在了 CPU 的 lldt 寄存器中。

```c
#define lldt(n) __asm__("lldt %%ax"::"a" (_LDT(n)))

void sched_init(void) {
    ...
    lldt(0);
    ...
}
```

而整个 GDT 与 LDT 表的设计，经过整个 [第一部分 进入内核前的苦力活]() 和 [第二部分 大战前期的初始化工作]() 的设计后，成了这个样子。

![图片](assets/640-1705909277238-rs.png)

所以，一目了然。



再看这行代码，把 EIP 寄存器赋值为了那行标号的地址。

```c
#define move_to_user_mode() \
_ams {
...
    _asm push offset l1\
    _asm iretd /* 执行中断返回指令*/ \
...
}
```

这里刚好设置的是下面**标号 l1 的位置**，所以 **iretd 之后 CPU 就乖乖去那里执行**了。

所以其实从效果上看，就是顺序往下执行，只不过利用了 iretd 做了些特权级转换等工作。

同理，这里的栈段 ss 和数据段 ds，都被赋值为了 17h，大家可以展开二进制算一下，他们又是什么特权级，对应的描述符又是谁。







### 22.7 总结

所以其实，最终效果上看**就是按顺序执行了我们所写的指令**，仿佛没有经过什么中断和中断返回的过程，但却**通过中断返回实现了特权级的翻转**，也就是从**内核态变为了用户态**，顺便**设置了栈段、代码段和数据段的基地址。**



好了，我们兜兜转转终于把这个 mov_to_user_mode 讲完了，特权级这块的检查细节非常繁琐，为了理解操作系统，我们只需要暂且记住如下一句话就好了：

**数据访问只能高特权级访问低特权级，代码跳转只能同特权级跳转，要想实现特权级转换，可以通过中断和中断返回来实现。**









## 23 设计进程调度

### 23.1 引言

书接上回，上回书咱们说到，操作系统通过 **move_to_user_mode** 方法，通过伪造一个中断和中断返回，巧妙地从内核态切换到了用户态。

今天，本来应该再往下讲 fork。

但这个是创建新进程的过程，是一个很能体现操作系统设计的地方。

所以我们先别急着看代码，我们今天就头脑风暴一下，就是**如果让你来设计整个进程调度**，你会怎么搞？



进程调度本质是什么？很简单，假如有三段代码被加载到内存中。

![图片](assets/640-1705912084565-rs.png)

进程调度就是让 CPU 一会去程序 1 的位置处运行一段时间，一会去程序 2 的位置处运行一段时间。







### 23.2 整体流程设计

如何做到刚刚说的，一会去这运行，一会去那运行？

**第一种办法**就是，程序 1 的代码里，每隔几行就写一段代码，主动放弃自己的执行权，跳转到程序 2 的地方运行。然后程序 2 也是如此。

但这种依靠程序自己的办法肯定不靠谱。



所以**第二种办法**就是，由一个不受任何程序控制的，**第三方的不可抗力**，每隔一段时间就中断一下 CPU 的运行，然后跳转到**一个特殊的程序**那里，这个程序通过某种方式获取到 **CPU 下一个要运行的程序的地址**，然后**跳转过去**。

- 这个每隔一段时间就**中断 CPU 的不可抗力**，就是由定时器触发的**时钟中断**。

  不知道你是否还记得，这个定时器和时钟中断，早在 [第18回 | 大名鼎鼎的进程调度就是从这里开始的]() 里讲的 **sched_init** 函数里就搞定了。

  ![图片](assets/640-1705912432190.gif)

- 而那个特殊的程序，就是具体的**进程调度函数**了。



好了，整个流程就这样处理完了，那么应该设计什么样的**数据结构**，来支持这个流程呢？不妨假设这个结构叫 **task_struct**。

```c
struct task_struct {
    ?
} 
```

换句话说，你总得有**一个结构**来记录各个**进程的信息**，比如它上一次执行到哪里了，要不 CPU 就算决定好了要跳转到你这个进程上运行，具体跳到哪一行运行，总得有个地方存吧？

我们一个个问题抛开来看。







### 23.3 上下文环境

每个程序最终的本质就是**执行指令**。这个过程会涉及**寄存器**，**内存**和**外设端口**。

内存还有可能设计成相互错开的，互不干扰，比如进程 1 你就用 0~1K 的内存空间，进程 2 就用 1K~2K 的内存空间，咱谁也别影响谁。

虽然有点浪费空间，而且对程序员十分不友好，但起码还是能实现的。

不过寄存器一共就那么点，肯定做不到互不干扰，可能一个进程就把寄存器全用上了，那其他进程咋整。

![图片](assets/640-1705914173958.png)

比如程序 1 刚刚往 eax 写入一个值，准备用，这时切换到进程 2 了，又往 eax 里写入了一个值。那么之后再切回进程 1 的时候，就出错了。



所以最稳妥的做法就是，**每次切换进程时，都把当前这些寄存器的值存到一个地方**，以便之后切换回来的时候恢复。

Linux 0.11 就是这样做的，每个进程的结构 task_struct 里面，有一个叫 **tss** 的结构，存储的就是 CPU 这些**寄存器**的信息。

```c
struct task_struct {
    ...
    struct tss_struct tss;
}

struct tss_struct {
    long    back_link;  /* 16 high bits zero */
    long    esp0;
    long    ss0;        /* 16 high bits zero */
    long    esp1;
    long    ss1;        /* 16 high bits zero */
    long    esp2;
    long    ss2;        /* 16 high bits zero */
    long    cr3;
    long    eip;
    long    eflags;
    long    eax,ecx,edx,ebx;
    long    esp;
    long    ebp;
    long    esi;
    long    edi;
    long    es;     /* 16 high bits zero */
    long    cs;     /* 16 high bits zero */
    long    ss;     /* 16 high bits zero */
    long    ds;     /* 16 high bits zero */
    long    fs;     /* 16 high bits zero */
    long    gs;     /* 16 high bits zero */
    long    ldt;        /* 16 high bits zero */
    long    trace_bitmap;   /* bits: trace 0, bitmap 16-31 */
    struct i387_struct i387;
};
```

这里提个细节。

你发现 tss 结构里还有个 **cr3** 不？它表示 cr3 寄存器里存的值，而 cr3 寄存器是指向页目录表首地址的。

![图片](assets/640-1705914275516-rs.png)

那么指向不同的页目录表，整个页表结构就是完全不同的一套，那么线性地址到物理地址的映射关系就有能力做到不同。

也就是说，在我们刚刚假设的理想情况下，不同程序用不同的内存地址可以做到内存互不干扰。

但是有了这个 cr3 字段，就完全可以无需由各个进程自己保证不和其他进程使用的内存冲突，因为只要建立不同的映射关系即可，由操作系统来建立不同的页目录表并替换 cr3 寄存器即可。

这也可以理解为，保存了**内存映射的上下文信息**。

当然 Linux 0.11 并不是通过替换 cr3 寄存器来实现内存互不干扰的，它的实现更为简单，这是后话了。







### 23.4 运行时间信息

如何判断一个进程该让出 CPU 了，切换到下一个进程呢？

总不能是每次时钟中断时都切换一次吧？一来这样不灵活，二来这完全依赖时钟中断的频率，有点危险。

所以一个好的办法就是，给进程一个属性，叫**剩余时间片**，每次时钟中断来了之后都 **-1**，如果减到 0 了，就触发切换进程的操作。

在 Linux 0.11 里，这个属性就是 **counter**。

```c
struct task_struct {
    ...
    long counter;
    ...
    struct tss_struct tss;
}
```

而他的用法也非常简单，就是每次中断都判断一下是否到 0 了。

```c
void do_timer(long cpl) {
    ...
    // 当前线程还有剩余时间片，直接返回
    if ((--current->counter)>0) return;
    // 若没有剩余时间片，调度
    schedule();
}
```

如果还没到 0，就直接返回，相当于这次时钟中断什么也没做，仅仅是给当前进程的时间片属性做了 -1 操作。

如果已经到 0 了，就触发**进程调度**，选择下一个进程并使 CPU 跳转到那里运行

进程调度的逻辑就是在 **schedule** 函数里，怎么调，我们先不管。







### 23.4 优先级

上面那个 counter 一开始的时候该是多少呢？而且随着 counter 不断递减，减到 0 时，下一轮回中这个 counter 应该赋予什么值呢？

其实这俩问题都是一个问题，就是 **counter 的初始化**问题，也需要有一个属性来记录这个值。

往宏观想一下，这个值越大，那么 counter 就越大，那么每次轮到这个进程时，它在 CPU 中运行的时间就越长，也就是这个进程比其他进程得到了更多 CPU 运行的时间。

那我们可以把这个值称为**优先级**，是不是很形象。

```c
struct task_struct {
    ...
    long counter;
    long priority;
    ...
    struct tss_struct tss;
}
```

**每次一个进程初始化时**，都把 **counter 赋值为这个 priority**，而且当 counter 减为 0 时，下一次分配时间片，也赋值为这个。

其实叫啥都行，反正就是这么用的，就叫优先级吧。







### 23.5 进程状态

其实我们有了上面那三个信息，就已经可以完成进程的调度了。

甚至如果你的操作系统让所有进程都得到同样的运行时间，连 counter 和 priority 都不用记录，就操作系统自己定一个固定值一直递减，减到 0 了就随机切一个新进程。

这样就仅仅维护好寄存器的上下文信息 tss 就好了。

但我们总要不断优化以适应不同场景的用户需求的，那我们再优化一个细节。



很简单的一个场景，一个进程中有一个读取硬盘的操作，发起读请求后，要等好久才能得到硬盘的中断信号。

那这个时间其实该进程再占用着 CPU 也没用，此时就可以选择**主动放弃 CPU 执行权，然后再把自己的状态标记为等待中。**

意思是告诉进程调度的代码，先别调度我，因为我还在等硬盘的中断，现在轮到我了也没用，把机会给别人吧。

那这个状态可以记录一个属性了，叫 **state**，记录了此时**进程的状态**。

```c
struct task_struct {
    long state;
    long counter;
    long priority;
    ...
    struct tss_struct tss;
}
```

而这个进程的状态在 Linux 0.11 里有这么五种。

```c
#define TASK_RUNNING          0
#define TASK_INTERRUPTIBLE    1
#define TASK_UNINTERRUPTIBLE  2
#define TASK_ZOMBIE           3
#define TASK_STOPPED          4
```

好了，目前我们这几个字段，就已经可以完成简单的进程调度任务了。

有表示状态的 **state**，表示剩余时间片的 **counter**，表示优先级的 **priority**，和表示上下文信息的 **tss**。



好了，今天我们完全由自己从零到有设计出了进程调度的大体流程，以及它需要的数据结构。

我们知道了**进程调度**的开始，要从**一次定时器滴答**来触发，通过**时钟中断处理函数**走到**进程调度函数**，然后去**进程的结构 task_struct** 中取出所需的数据，进行**策略计算**，并挑选出**下一个可以得到 CPU 运行的进程**，跳转过去。









## 24 从一次定时器滴答来看进程调度

### 24.1 一次进程调度全过程

上回书咱们说到，我们完全由自己从零到有设计出了进程调度的大体流程，以及它需要的数据结构。

```c
struct task_struct {
    long state;
    long counter;
    long priority;
    ...
    struct tss_struct tss;
}
```

这一讲，我们从**一次定时器滴答**出发，看看一次 Linux 0.11 的**进程调度的全过程**。



还记得我们在 [第18回 | 大名鼎鼎的进程调度就是从这里开始的]() **sched_init** 的时候，开启了**定时器**吧？这个定时器每隔一段时间就会向 CPU 发起一个中断信号。

![图片](assets/640-1705915228692.gif)

这个间隔时间被设置为 10 ms，也就是 100 Hz。

``kernel\sched.c``

```c
#define HZ 100
```

发起的中断叫**时钟中断**，其中断向量号被设置为了 **0x20**。

还记得我们在 **sched_init** 里设置的时钟中断和对应的中断处理函数吧？

`kernel\sched.c`

```c
set_intr_gate(0x20, &timer_interrupt);
```

这样，当时钟中断，也就是 0x20 号中断来临时，CPU 会查找中断向量表中 0x20 处的函数地址，即中断处理函数，并跳转过去执行。

这个中断处理函数就是 **timer_interrupt**，是用汇编语言写的。

`kernel\system_call.s`

```assembly
_timer_interrupt:
    ...
    // 增加系统滴答数
    incl _jiffies
    ...
    // 调用函数 do_timer
    call _do_timer
    ...
```

这个函数做了两件事，一个是将**系统滴答数**这个变量 **jiffies** 加一，一个是调用了另一个函数 **do_timer**。

`kernel\sched.c`

```c
void do_timer(long cpl) {
    ...
    // 当前线程还有剩余时间片，直接返回
    if ((--current->counter)>0) return;
    // 若没有剩余时间片，调度
    schedule();
}
```

do_timer 最重要的部分就是上面这段代码，非常简单。

首先将当先进程的时间片 -1，然后判断：

- 如果时间片仍然大于零，则什么都不做直接返回。 
- 如果时间片已经为零，则调用 `schedule()`，很明显，这就是进行进程调度的主干。







### 24.2 schedule()流程

对`schedule()`做个不严谨的简化，你就明白了

`kernel\sched.c`

```c
void schedule(void) {
    int next = get_max_counter_and_runnable_thread();
    refresh_all_thread_counter();
    switch_to(next);
}
```

看到没，就剩这么点了。

很简答，这个函数就做了三件事：

1. 拿到剩余时间片（`counter`的值）最大且在 runnable 状态（`state = 0`）的**进程号 next**。
2. 如果所有 runnable 进程时间片都为 0，则将所有进程（注意不仅仅是 runnable 的进程）的 counter 重新赋值（counter = counter/2 + priority），然后再次执行步骤 1。
3. 最后拿到了一个**进程号 next**，调用了` switch_to(next)` 这个方法，就**切换到了这个进程去执行**了。

看 `switch_to` 方法，是用内联汇编语句写的。

`include\linux\sched.h`

```c
#define switch_to(n) {\
struct {long a,b;} __tmp; \
__asm__("cmpl %%ecx,_current\n\t" \
    "je 1f\n\t" \
    "movw %%dx,%1\n\t" \
    "xchgl %%ecx,_current\n\t" \
    "ljmp %0\n\t" \
    "cmpl %%ecx,_last_task_used_math\n\t" \
    "jne 1f\n\t" \
    "clts\n" \
    "1:" \
    ::"m" (*&__tmp.a),"m" (*&__tmp.b), \
    "d" (_TSS(n)),"c" ((long) task[n])); \
}
```

这段话就是进程切换的最最最最底层的代码了。

看不懂没关系，其实主要就干了一件事，**就是 ljmp 到新进程的 tss 段处。**



啥意思？

**CPU 规定，如果 ljmp 指令后面跟的是一个 tss 段**，那么，

- 会由硬件将当前各个寄存器的值保存在当前进程的 tss 中
- 并将新进程的 tss 信息加载到各个寄存器

![图片](assets/640-1705916169982.png)

简单说就是，**保存当前进程上下文，恢复下一个进程的上下文，跳过去**！





### 24.3 整体流水账

看，不知不觉，我们上一讲和本讲开头提到的那些进程数据结构的字段，就都用上了。

```
struct task_struct {
    long state;
    long counter;
    long priority;
    ...
    struct tss_struct tss;
}
```

至此，我们梳理完了一个进程切换的整条链路，来回顾一下。

- 罪魁祸首的，就是那个每 10ms 触发一次的**定时器滴答**。
- 而这个滴答将会给 CPU 产生一个**时钟中断信号**。
- 而这个中断信号会使 CPU 查找**中断向量表**，找到操作系统写好的一个**时钟中断处理函数 do_timer**。
- do_timer 会首先将当前进程的 **counter 变量** -1，如果 counter 此时仍然大于 0，则**就此结束**。
- 但如果 counter = 0 了，就开始**进行进程的调度**。
- **进程调度**就是找到所有处于 RUNNABLE 状态的进程，并找到一个 counter 值最大的进程，把它丢进 **switch_to 函数**的入参里。
- **switch_to** 这个终极函数，会**保存当前进程上下文，恢复要跳转到的这个进程的上下文**，同时使得 CPU **跳转到这个进程的偏移地址处**。 
- 接着，这个进程就舒舒服服地运行了起来，等待着下一次时钟中断的来临。











## 25 通过fork看一次系统调用

### 25.1 fork函数

上回书咱们说到，我们通过自己设计了一遍进程调度，又看了一次 Linux 0.11 的进程调度的全过程。有了这两回做铺垫，我们下一回就该非常自信地回到我们的主流程！

```c
void main(void) {
    ...    
    move_to_user_mode();
    if (!fork()) {
        init();
    }
    for(;;) pause();
}
```

也就是这个 fork 函数干了啥？

 

这个 fork 函数稍稍绕了点，我们看如下代码。

`init\main.c`

```c
static _inline _syscall0(int,fork);


#define _syscall0(type,name) \
type name(void) \
{ \
    volatile long __res; \
    _asm { \
        _asm mov eax,__NR_##name \
        _asm int 80h \
        _asm mov __res,eax \
    } \
    if (__res >= 0) \
        return (type) __res; \
    errno = -__res; \
    return -1; \
}
```

把宏定义都展开，其实就相当于**定义了一个函数**。

```c
int fork(void) {
     volatile long __res;
    _asm {
        _asm mov eax,__NR_fork
        _asm int 80h
        _asm mov __res,eax
    }
    if (__res >= 0)
        return (void) __res;
    errno = -__res;
    return -1;
}
```







### 25.2 fork调用链

具体看一下 fork 函数里面的代码，关键指令就是一个 0x80 号软中断的触发，**int 80h**。

其中还有一个 eax 寄存器里的参数是 **__NR_fork**，这也是个宏定义，值是 **2**

OK，还记得 0x80 号中断的处理函数么？这个是我们在 [第18回 | 大名鼎鼎的进程调度就是从这里开始的]() **sched_init** 里面设置的。

```c
set_system_gate(0x80, &system_call);
```

看这个 system_call 的汇编代码，我们发现这么一行。

`kernel\system_call.s`

```assembly
_system_call:
    ...
    call [_sys_call_table + eax*4]
    ...
```

刚刚那个值就用上了，eax 寄存器里的值是 2，所以这个就是在这个 **sys_call_table** 表里找下标 2 位置处的函数，然后跳转过去。

那我们接着看 sys_call_table 是个啥。

`include\linux\sys.h`

```c
fn_ptr sys_call_table[] = { sys_setup, sys_exit, sys_fork, sys_read,
sys_write, sys_open, sys_close, sys_waitpid, sys_creat, sys_link,
sys_unlink, sys_execve, sys_chdir, sys_time, sys_mknod, sys_chmod,
sys_chown, sys_break, sys_stat, sys_lseek, sys_getpid, sys_mount,
sys_umount, sys_setuid, sys_getuid, sys_stime, sys_ptrace, sys_alarm,
sys_fstat, sys_pause, sys_utime, sys_stty, sys_gtty, sys_access,
sys_nice, sys_ftime, sys_sync, sys_kill, sys_rename, sys_mkdir,
sys_rmdir, sys_dup, sys_pipe, sys_times, sys_prof, sys_brk, sys_setgid,
sys_getgid, sys_signal, sys_geteuid, sys_getegid, sys_acct, sys_phys,
sys_lock, sys_ioctl, sys_fcntl, sys_mpx, sys_setpgid, sys_ulimit,
sys_uname, sys_umask, sys_chroot, sys_ustat, sys_dup2, sys_getppid,
sys_getpgrp, sys_setsid, sys_sigaction, sys_sgetmask, sys_ssetmask,
sys_setreuid,sys_setregid };
```

看到没，就是各种函数指针组成的一个数组，说白了就是个系统调用函数表。

那下标 2 位置处是啥？从第零项开始数，第二项就是 **sys_fork** 函数！

至此，我们终于找到了 fork 函数，通过系统调用这个中断，最终走到内核层面的函数是什么，就是 **sys_fork**。

`kernel\system_call.s`

```assembly
_sys_fork:
    call _find_empty_process
    testl %eax,%eax
    js 1f
    push %gs
    pushl %esi
    pushl %edi
    pushl %ebp
    pushl %eax
    call _copy_process
    addl $20,%esp
1:  ret
```





从这讲的探索我们也可以看出，操作系统通过**系统调用**，提供给用户态可用的功能，都暴露在**sys_call_table** 里了。

系统调用统一通过 **int 0x80** 中断来进入，具体调用这个表里的哪个功能函数，就由 **eax** 寄存器传过来，这里的值是个数组索引的下标，通过这个下标就可以找到在 sys_call_table 这个数组里的具体函数。

同时也可以看出，**用户进程调用内核的功能**，可以直接通过写一句 int 0x80 汇编指令，并且给 eax 赋值，当然这样就比较麻烦。

所以也可以直接调用 fork 这样的包装好的方法，而这个方法里本质也是 int 0x80 以及 eax 赋值而已。

**一次系统调用的流程和原理：**

![图片](assets/640-1705917465362.png)









### Notes-syscall详解

刚刚定义 fork 的系统调用模板函数时，用的是 **syscall0**，其实这个表示参数个数为 0，也就是 sys_fork 函数并不需要任何参数。



所以其实，在 unistd.h 头文件里，还定义了 syscall0 ~ syscall3 一共四个宏。

`include\unistd.h`

```c
#define _syscall0(type,name)
#define _syscall1(type,name,atype,a)
#define _syscall2(type,name,atype,a,btype,b)
#define _syscall3(type,name,atype,a,btype,b,ctype,c)
```

看都能看出来，其实 **syscall1** 就表示有**一个参数**，**syscall2** 就表示有**两个参数**。



那这些参数放在哪里了呢？总得有个约定的地方吧？

我们看一个今后要讲的重点函数，**execve**，是一个通常和 fork 在一起配合的变身函数，在之后的进程 1 创建进程 2 的过程中，就是这样玩的。

`init\main.c`

```c
void init(void) {
    ...
    if (!(pid=fork())) {
        ...
        execve("/bin/sh",argv_rc,envp_rc);
        ...
    }
}
```

当然我们的重点不是研究这个函数的作用，仅仅把它当做研究 syscall3 的一个例子，因为它的宏定义就是 **syscall3**。

`include\unistd.h`

```c
// execve("/bin/sh",argv_rc,envp_rc);

_syscall3(int,execve,const char *,file,char **,argv,char **,envp)

#define _syscall3(type,name,atype,a,btype,b,ctype,c) \
type name(atype a,btype b,ctype c) { \
    volatile long __res; \
    _asm { \
        _asm mov eax,__NR_##name \
        _asm mov ebx,a \
        _asm mov ecx,b \
        _asm mov edx,c \
        _asm int 80h \
        _asm mov __res,eax\
    } \
    if (__res >= 0) \
        return (type) __res; \
    errno = -__res; \
    return -1; \
}
```

可以看出，**参数 a 被放在了 ebx 寄存器，参数 b 被放在了 ecx 寄存器，参数 c 被放在了 edx 寄存器**。



我们再打开 system_call 的代码，刚刚我们只看了它的关键一行，就是去系统调用表里找函数。

`kernel\system_call.s`

```assembly
_system_call:
    ...
    call [_sys_call_table + eax*4]
    ...
```

我们再看看全貌。

```assembly
_system_call:
    cmpl $nr_system_calls-1,%eax
    ja bad_sys_call
    push %ds
    push %es
    push %fs
    pushl %edx
    pushl %ecx      # push %ebx,%ecx,%edx as parameters
    pushl %ebx      # to the system call
    movl $0x10,%edx     # set up ds,es to kernel space
    mov %dx,%ds
    mov %dx,%es
    movl $0x17,%edx     # fs points to local data space
    mov %dx,%fs
    call _sys_call_table(,%eax,4)
    pushl %eax
    movl _current,%eax
    cmpl $0,state(%eax)     # state
    jne reschedule
    cmpl $0,counter(%eax)       # counter
    je reschedule
ret_from_sys_call:
    movl _current,%eax      # task[0] cannot have signals
    cmpl _task,%eax
    je 3f
    cmpw $0x0f,CS(%esp)     # was old code segment supervisor ?
    jne 3f
    cmpw $0x17,OLDSS(%esp)      # was stack segment = 0x17 ?
    jne 3f
    movl signal(%eax),%ebx
    movl blocked(%eax),%ecx
    notl %ecx
    andl %ebx,%ecx
    bsfl %ecx,%ecx
    je 3f
    btrl %ecx,%ebx
    movl %ebx,signal(%eax)
    incl %ecx
    pushl %ecx
    call _do_signal
    popl %eax
3:  popl %eax
    popl %ebx
    popl %ecx
    popl %edx
    pop %fs
    pop %es
    pop %ds
    iret
```

又被吓到了是不是？



别怕，我们只关注压栈的情况，还记不记得在 [一个新进程的诞生（二）从内核态到用户态]() 讲中，我们聊到触发了中断后，CPU 会自动帮我们做如下压栈操作。

![图片](assets/640-1705922889298-rs.png)

因为 system_call 是通过 int 80h 这个软中断进来的，所以也属于中断的一种，具体说是属于特权级发生变化的，且没有错误码情况的中断，所以在这之前栈已经被压了 **SS、ESP、EFLAGS、CS、EIP** 这些值。

接下来 system_call 又压入了一些值，具体说来有 **ds、es、fs、edx、ecx、ebx、eax**。

如果你看源码费劲，得不出我上述结论，那你可以看 `system_call.s` 上面的注释，Linus 作者已经很贴心地给你写出了此时的堆栈状态。

`kernel\system_call.s`

```assembly
/*
 * Stack layout in 'ret_from_system_call':
 *
 *   0(%esp) - %eax
 *   4(%esp) - %ebx
 *   8(%esp) - %ecx
 *   C(%esp) - %edx
 *  10(%esp) - %fs
 *  14(%esp) - %es
 *  18(%esp) - %ds
 *  1C(%esp) - %eip
 *  20(%esp) - %cs
 *  24(%esp) - %eflags
 *  28(%esp) - %oldesp
 *  2C(%esp) - %oldss
 */
```

看，就是 CPU 中断压入的 5 个值，加上 system_call 手动压入的 7 个值。

所以之后，中断处理程序如果有需要的话，就可以从这里取出它想要的值，包括 CPU 压入的那五个值，或者 system_call 手动压入的 7 个值。

比如 **sys_execve** 这个中断处理函数，一开始就取走了位于栈顶 0x1C 位置处的 EIP 的值。

```assembly
EIP = 0x1C
_sys_execve:
    lea EIP(%esp),%eax
    pushl %eax
    call _do_execve
    addl $4,%esp
    ret
```

随后在 **do_execve** 函数中，又通过 C 语言函数调用的约定，取走了 **filename，argv，envp** 等参数。

```c
int do_execve(
        unsigned long * eip,
        long tmp,
        char * filename,
        char ** argv,
        char ** envp) {
    ...
}
```

具体这个函数的详细流程和作用，将会在第四部分的 shell 程序装载章节讲到。











## 26 fork中进程基本信息的复制

### 26.1 sys_fork整体

fork 触发系统调用中断，最终调用到了 sys_fork 函数，借这个过程介绍了一次**系统调用**的流程。

![图片](assets/640-1705923392084.png)

**fork** 函数的原理，实际上就是 **sys_fork** 函数干了啥。

```assembly
_sys_fork:
    call _find_empty_process
    testl %eax,%eax
    js 1f
    push %gs
    pushl %esi
    pushl %edi
    pushl %ebp
    pushl %eax
    call _copy_process
    addl $20,%esp
1:  ret
```

其实就是调用了两个函数。我们先从方法名直接翻译一下，猜猜意思。

- 先是 **find_empty_process**，就是找到空闲的进程槽位。 

  那妥了，这个方法的意思非常简单，因为存储进程的数据结构是一个 **task[64] 数组**

  [^task数组]: 这个是在之前 第18回 | 大名鼎鼎的进程调度就是从这里开始的**sched_init** 函数的时候设置的。



  ![图片](assets/640-1705923788047-rs.png)

  就是先**在这个数组中找一个空闲的位置**，准备存一个新的进程的结构 **task_struct**

  ```c
  struct task_struct {
      long state;
      long counter;
      long priority;
      ...
      struct tss_struct tss;
  }
  ```

- 然后 **copy_process**，就是复制进程。

  这个结构(`task_struct`)各个字段具体赋什么值呢？

  通过 **copy_process** 这个名字我们知道，就是复制原来的进程，也就是当前进程。

  **当前只有一个进程**，就是数组中位置 0 处的 **init_task.init**，也就是**零号进程**，那自然就复制它咯。







### 26.2 find_empty_process

先来 **find_empty_process**。

`kernel\fork.c`

```c
long last_pid = 0;

int find_empty_process(void) {
    int i;
    repeat:
        if ((++last_pid)<0) last_pid=1;
        for(i=0 ; i<64 ; i++)
            if (task[i] && task[i]->pid == last_pid) goto repeat;
    for(i=1 ; i<64; i++)
        if (!task[i])
            return i;
    return -EAGAIN;
}
```

一共三步，很简单。

- **第一步**，判断 ++last_pid 是不是小于零了，小于零说明已经超过 long 的最大值了，重新赋值为 1，起到一个保护作用，这没什么好说的。
- **第二步**，一个 for 循环，看看刚刚的 last_pid 在所有 task[] 数组中，是否已经被某进程占用了。如果被占用了，那就重复执行，再次加一，然后再次判断，直到找到一个 pid 号没有被任何进程用为止。
- **第三步**，又是个 for 循环，刚刚已经**找到一个可用的 pid 号**了，那这一步就是再次遍历这个 task[] 试图**找到一个空闲项**，找到了就返回素组索引下标。

**最终，这个方法就返回 task[] 数组的索引，表示找到了一个空闲项**，之后就开始往这里塞一个新的进程吧。

由于我们现在只有 0 号进程，且 task[] 除了 0 号索引位置，其他地方都是空的，所以这个方法运行完，last_**pid 就是 1，也就是新进程被分配的 pid 就是 1**，然后即将要加入的 task[] 数组的索引位置，也是 1。









### 26.3 copy_process

好的，那我们接下来就看，怎么构造这个进程结构，塞到这个 1 索引位置的 task[] 中？

来看 **copy_process** 方法。大部分都是 tss 结构的复制，以及一些无关紧要的分支，看我简化下。

`kernel\fork.c`

```c
int copy_process(int nr, ...) {
    struct task_struct p = 
        (struct task_struct *) get_free_page();
    task[nr] = p;
    *p = *current;

    p->state = TASK_UNINTERRUPTIBLE;
    p->pid = last_pid;
    p->counter = p->priority;
    ..
    p->tss.edx = edx;
    p->tss.ebx = ebx;
    p->tss.esp = esp;
    ...
    copy_mem(nr,p);
    ...
    set_tss_desc(gdt+(nr<<1)+FIRST_TSS_ENTRY,&(p->tss));
    set_ldt_desc(gdt+(nr<<1)+FIRST_LDT_ENTRY,&(p->ldt));
    p->state = TASK_RUNNING;
    return last_pid;
}
```

这个函数本来就是 fork 的难点了，所以我们慢慢来。



首先 **get_free_page** 会在主内存末端申请一个空闲页面，还记得我们之前在 [第13回 内存初始化 mem_init]() 里是怎么管理内存的吧？

![图片](assets/640-1705925681770-rs.png)

那 get_free_page 这个函数就很简单了，**就是遍历 mem_map[] 这个数组，找出值为零的项，就表示找到了空闲的一页内存**。然后把该项置为 1，表示该页已经被使用。最后，算出这个页的内存起始地址，返回。

然后，拿到的这个内存起始地址，就给了 task_struct 结构的 p。

```c
int copy_process(int nr, ...) {
    struct task_struct p = 
        (struct task_struct *) get_free_page();
    task[nr] = p;
    *p = *current;
    ...
}
```

于是乎，一个进程结构 task_struct 就在内存中有了一块空间，但此时还没有赋值具体的字段。



首先将这个 p 记录在进程管理结构 task[] 中。

然后下一句 `*p = *current` 很简单，**就是把当前进程，也就是 0 号进程的 task_struct 的全部值都复制给即将创建的进程 p**，目前它们两者就完全一样了。

嗯，这就附上值了，就完全复制之前的进程的 task_struct 而已，很粗暴。

最后的内存布局的效果就是这样。

![图片](assets/640-1705925861034-rs.png)

然后，进程 1 和进程 0 目前是完全复制的关系，**但有一些值是需要个性化处理的**，下面的代码就是把这些不一样的值覆盖掉。

```c
int copy_process(int nr, ...) {
    ...
    p->state = TASK_UNINTERRUPTIBLE;
    p->pid = last_pid;
    p->counter = p->priority;
    ..
    p->tss.edx = edx;
    p->tss.ebx = ebx;
    p->tss.esp = esp;
    ...
    p->tss.esp0 = PAGE_SIZE + (long) p;
    p->tss.ss0 = 0x10;
    ...
}
```

不一样的值，

- 一部分是 **state**，**pid**，**counter** 这种**进程的元信息**，
- 另一部分是 **tss** 里面保存的各种寄存器的信息，即**上下文**。

这里有两个寄存器的值的赋值有些特殊，就是 `ss0` 和 `esp0`，这个表示 **0 特权级**也就是**内核态时**的 `ss:esp` 的指向。

根据代码我们得知，其含义是将代码在**内核态时使用的堆栈栈顶指针**指向进程 `task_struct` 所在的 4K **内存页的最顶端**，而且之后的每个进程都是这样被设置的。

![图片](assets/640-1705926245101-rs.png)

好了，进程槽位的申请，以及基本信息的复制，就讲完了。

今天就这么点内容，**就是内存中找个地方存一个 task_struct 结构的东东，并添加到 task[] 数组里的空闲位置处，这个东东的具体字段赋值的大部分都是复制原来进程的**。









## 27 透过 fork 来看进程的内存规划

上回书咱们说到，**fork** 函数为新的进程（进程 1）申请了槽位，并把全部 **task_struct** 结构的值都从进程零复制了过来。

![图片](assets/640-1705926824655-rs.png)

之后，覆盖了新进程自己的基本信息，包括元信息和 tss 里的寄存器信息。

```c
int copy_process(int nr, ...) {
    ...
    p->state = TASK_UNINTERRUPTIBLE;
    p->pid = last_pid;
    p->counter = p->priority;
    ..
    p->tss.edx = edx;
    p->tss.ebx = ebx;
    p->tss.esp = esp;
    ...
}
```





### 27.1 copy_mem

这可以说将 fork 函数的一半都讲完了，那我们今天展开讲讲另一半，也就是 **copy_mem** 函数。

`kernel\fork.c`

```c
int copy_process(int nr, ...) {
    ...
    copy_mem(nr,p);
    ...
}
```

这将会决定**进程之间的内存规划问题**，整个函数不长，我们还是试着先直译一下。

`kernel\fork.c`

```c
int copy_mem(int nr,struct task_struct * p) {
    // 局部描述符表 LDT 赋值
    unsigned long old_data_base,new_data_base,data_limit;
    unsigned long old_code_base,new_code_base,code_limit;
    code_limit = get_limit(0x0f);
    data_limit = get_limit(0x17);
    new_code_base = nr * 0x4000000;
    new_data_base = nr * 0x4000000;
    set_base(p->ldt[1],new_code_base);
    set_base(p->ldt[2],new_data_base);
    // 拷贝页表
    old_code_base = get_base(current->ldt[1]);
    old_data_base = get_base(current->ldt[2]);
    copy_page_tables(old_data_base,new_data_base,data_limit);
    return 0;
}
```

看，其实就是**新进程 LDT 表项的赋值，以及页表的拷贝**。







### 27.2 LDT的赋值











### 27.3 页表的复制









### 27.4 小结































